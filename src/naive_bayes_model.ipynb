{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define local constants\n",
    "Change these constants based on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Input data path\n",
    "INPUT_TRAINING_FILE = \"../data/preprocessed_data/training_dataset.csv\"\n",
    "# Evaluation dataset should always stay the same\n",
    "INPUT_EVALUATION_FILE = \"../data/preprocessed_data/evaluation_dataset.csv\"\n",
    "\n",
    "# Output parameters\n",
    "METHOD_NAME = \"tf_idf_multinomial_nb\"\n",
    "PREPROCESSOR_NAME = \"baseline_and_bayess_specific\"\n",
    "OUTPUT_MODEL = f\"../data/models/{METHOD_NAME}_model.pkl\"\n",
    "OUTPUT_RESULTS = f\"../data/results/{METHOD_NAME}_model.txt\"\n",
    "\n",
    "# Hyper parameter alternatives\n",
    "HYPER_PARAMETER_MIN_DF = list(range(0, 100, 10))\n",
    "HYPER_PARAMETER_MAX_DF = list(numpy.arange(0.01, 0.10, 0.02))\n",
    "HYPER_PARAMETER_MAX_FEATURES = list(range(5000, 100000, 5000))\n",
    "HYPER_PARAMETER_BINARY = [False, True]\n",
    "HYPER_PARAMETER_USE_IDF = [False, True]\n",
    "\n",
    "# Hyper parameter optimization parameters\n",
    "HYPER_PARAMETER_OPTIMIZATION_SCORING = \"accuracy\"\n",
    "HYPER_PARAMETER_OPTIMIZATION_CV = 5\n",
    "\n",
    "# Other constants\n",
    "LABELS = [\"negative\", \"positive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries for your machine learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets\n",
    "Note that the preprocessed data should contain at least the following fields:\n",
    "[prep_text],[sentiment]\n",
    "\n",
    "Loading training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_csv(INPUT_TRAINING_FILE, engine=\"python\", delimiter=\",\")\n",
    "training_tweets = training_dataset[\"prep_text\"].apply(\n",
    "    lambda tweet: str(tweet))\n",
    "training_sentiment_targets = training_dataset[\"sentiment\"].apply(\n",
    "    lambda sentiment: int(sentiment))\n",
    "\n",
    "evaluation_dataset = pd.read_csv(INPUT_EVALUATION_FILE, engine=\"python\", delimiter=\",\")\n",
    "evaluation_tweets = evaluation_dataset[\"prep_text\"].apply(\n",
    "    lambda tweet: str(tweet))\n",
    "evaluation_sentiment_targets = evaluation_dataset[\"sentiment\"].apply(\n",
    "    lambda sentiment: int(sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the preprocessor and do some preprocessing for the training dataset\n",
    "Preprocessing part should only include conversion techniques that are required by the algorithm. General preprocessing should be done in the separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove very short tweets from the training dataset\n",
    "mask = training_tweets.str.len() > 30\n",
    "training_tweets = training_tweets[mask]\n",
    "training_sentiment_targets = training_sentiment_targets[mask]\n",
    "\n",
    "# Define the count vectorizer with certain sanity limits\n",
    "preprocessor = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define rest of the pipeline\n",
    "Definition should include splitting of the data using cross validator and hyper parameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create internal pipeline\n",
    "classifier = MultinomialNB()\n",
    "pipeline = Pipeline(steps=[(\"preprocessing\", preprocessor), (\"classification\", classifier)])\n",
    "\n",
    "# Specify the tunable hyper parameters\n",
    "parameters = {\n",
    "    \"preprocessing__min_df\": HYPER_PARAMETER_MIN_DF,\n",
    "    \"preprocessing__max_df\": HYPER_PARAMETER_MAX_DF,\n",
    "    \"preprocessing__max_features\": HYPER_PARAMETER_MAX_FEATURES,\n",
    "    \"preprocessing__binary\": HYPER_PARAMETER_BINARY,\n",
    "    \"preprocessing__use_idf\": HYPER_PARAMETER_USE_IDF,\n",
    "}\n",
    "\n",
    "# Define KFold parameters\n",
    "cv = StratifiedKFold(n_splits=HYPER_PARAMETER_OPTIMIZATION_CV, shuffle=True, random_state=42)\n",
    "\n",
    "estimator = GridSearchCV(pipeline, parameters,\n",
    "    scoring=HYPER_PARAMETER_OPTIMIZATION_SCORING, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32055/257579218.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_sentiment_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    836\u001b[0m                     )\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimator.fit(training_tweets, training_sentiment_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all training data to calculate confusion matrix for training data\n",
    "training_estimates = estimator.predict(training_tweets)\n",
    "training_accuracy = accuracy_score(training_sentiment_targets, training_estimates)\n",
    "training_confusion_matrix = confusion_matrix(training_sentiment_targets, training_estimates)\n",
    "training_classification_report = classification_report(training_sentiment_targets, training_estimates, output_dict=True, target_names=LABELS)\n",
    "\n",
    "# Use model to estimate manually labeled evaluation Tweets\n",
    "evaluation_estimates = estimator.predict(evaluation_tweets)\n",
    "evaluation_accuracy = accuracy_score(evaluation_sentiment_targets, evaluation_estimates)\n",
    "evaluation_confusion_matrix = confusion_matrix(evaluation_sentiment_targets, evaluation_estimates)\n",
    "evaluation_classification_report = classification_report(evaluation_sentiment_targets, evaluation_estimates, output_dict=True, target_names=LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained model for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_MODEL, \"wb\") as handle:\n",
    "    pickle.dump(estimator, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result statistics\n",
    "These should be always saved in the same fashion, so the results can be compared between different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata:\n",
      "  estimator_name: MultinomialNB()\n",
      "  method_name: naive_bayes\n",
      "  parameter_optimization:\n",
      "    hyper_parameter_optimization: &id001\n",
      "      cv: 2\n",
      "      scoring: accuracy\n",
      "  preprocessor_name: baseline_and_bayess_specific\n",
      "parameters:\n",
      "  hyper_parameter_optimization_values: *id001\n",
      "  selected_hyper_parameters:\n",
      "    classification__fit_prior: true\n",
      "scores:\n",
      "  evaluation_scores:\n",
      "    accuracy: 0.8793969849246231\n",
      "    classification_report:\n",
      "      accuracy: 0.8793969849246231\n",
      "      macro avg:\n",
      "        f1-score: 0.8793939393939394\n",
      "        precision: 0.8795211153768439\n",
      "        recall: 0.8794444444444445\n",
      "        support: 199\n",
      "      negative:\n",
      "        f1-score: 0.88\n",
      "        precision: 0.8712871287128713\n",
      "        recall: 0.8888888888888888\n",
      "        support: 99\n",
      "      positive:\n",
      "        f1-score: 0.8787878787878789\n",
      "        precision: 0.8877551020408163\n",
      "        recall: 0.87\n",
      "        support: 100\n",
      "      weighted avg:\n",
      "        f1-score: 0.8793908938632556\n",
      "        precision: 0.8795624921942506\n",
      "        recall: 0.8793969849246231\n",
      "        support: 199\n",
      "    confusion_matrix:\n",
      "      false_negative: 11\n",
      "      false_positive: 13\n",
      "      true_negative: 88\n",
      "      true_positive: 87\n",
      "  training_scores:\n",
      "    accuracy: 0.6969525364632866\n",
      "    classification_report:\n",
      "      accuracy: 0.6969525364632866\n",
      "      macro avg:\n",
      "        f1-score: 0.6963958913361523\n",
      "        precision: 0.6990219491237124\n",
      "        recall: 0.6973090630900243\n",
      "        support: 1575146\n",
      "      negative:\n",
      "        f1-score: 0.7093958816309833\n",
      "        precision: 0.6767858988998326\n",
      "        recall: 0.7453074810386799\n",
      "        support: 781723\n",
      "      positive:\n",
      "        f1-score: 0.6833959010413213\n",
      "        precision: 0.721257999347592\n",
      "        recall: 0.6493106451413685\n",
      "        support: 793423\n",
      "      weighted avg:\n",
      "        f1-score: 0.6962993289308581\n",
      "        precision: 0.6991871159003917\n",
      "        recall: 0.6969525364632866\n",
      "        support: 1575146\n",
      "    confusion_matrix:\n",
      "      false_negative: 199099\n",
      "      false_positive: 278245\n",
      "      true_negative: 582624\n",
      "      true_positive: 515178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary object, where results will be accumulated\n",
    "result_dict= {}\n",
    "\n",
    "# Metadata section\n",
    "metadata_dict = {}\n",
    "metadata_dict[\"preprocessor_name\"] = PREPROCESSOR_NAME\n",
    "metadata_dict[\"method_name\"] = METHOD_NAME\n",
    "metadata_dict[\"estimator_name\"] = str(classifier)\n",
    "result_dict[\"metadata\"] = metadata_dict\n",
    "\n",
    "# Hyper parameter optimization values\n",
    "hyper_parameter_optimization_dict = {}\n",
    "hyper_parameter_optimization_dict[\"scoring\"] = HYPER_PARAMETER_OPTIMIZATION_SCORING\n",
    "hyper_parameter_optimization_dict[\"cv\"] = HYPER_PARAMETER_OPTIMIZATION_CV\n",
    "\n",
    "parameter_optimization_dict = {}\n",
    "parameter_optimization_dict[\"hyper_parameter_optimization\"] = hyper_parameter_optimization_dict\n",
    "metadata_dict[\"parameter_optimization\"] = parameter_optimization_dict\n",
    "\n",
    "# Save best parameters\n",
    "result_dict[\"best_parameters\"] = estimator.best_params_\n",
    "\n",
    "# Different kind of scores\n",
    "scores_dict = {}\n",
    "\n",
    "training_scores_dict = {}\n",
    "training_scores_dict[\"accuracy\"] = float(training_accuracy)\n",
    "training_confusion_matrix_dict = {}\n",
    "training_confusion_matrix_dict[\"true_negative\"] = int(training_confusion_matrix[0][0])\n",
    "training_confusion_matrix_dict[\"true_positive\"] = int(training_confusion_matrix[1][1])\n",
    "training_confusion_matrix_dict[\"false_negative\"] = int(training_confusion_matrix[0][1])\n",
    "training_confusion_matrix_dict[\"false_positive\"] = int(training_confusion_matrix[1][0])\n",
    "training_scores_dict[\"confusion_matrix\"] = training_confusion_matrix_dict\n",
    "training_scores_dict[\"classification_report\"] = training_classification_report\n",
    "scores_dict[\"training_scores\"] = training_scores_dict\n",
    "\n",
    "evaluation_scores_dict = {}\n",
    "evaluation_scores_dict[\"accuracy\"] = float(evaluation_accuracy)\n",
    "evaluation_confusion_matrix_dict = {}\n",
    "evaluation_confusion_matrix_dict[\"true_negative\"] = int(evaluation_confusion_matrix[0][0])\n",
    "evaluation_confusion_matrix_dict[\"true_positive\"] = int(evaluation_confusion_matrix[1][1])\n",
    "evaluation_confusion_matrix_dict[\"false_negative\"] = int(evaluation_confusion_matrix[0][1])\n",
    "evaluation_confusion_matrix_dict[\"false_positive\"] = int(evaluation_confusion_matrix[1][0])\n",
    "evaluation_scores_dict[\"confusion_matrix\"] = evaluation_confusion_matrix_dict\n",
    "evaluation_scores_dict[\"classification_report\"] = evaluation_classification_report\n",
    "scores_dict[\"evaluation_scores\"] = evaluation_scores_dict\n",
    "\n",
    "result_dict[\"scores\"] = scores_dict\n",
    "\n",
    "# Convert statistics to pretty YAML\n",
    "results = yaml.dump(result_dict)\n",
    "\n",
    "# Print results\n",
    "print(results)\n",
    "\n",
    "# Save results to the file\n",
    "with open(OUTPUT_RESULTS, \"w\") as file:\n",
    "    file.write(results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df7a9a5c37ddd52c7652a98db41541d6c37368917739ba33de99dadb21ef70fe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
