{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define local constants\n",
    "Change these constants based on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Input data path\n",
    "INPUT_TRAINING_FILE = \"../data/preprocessed_data/training_dataset.csv\"\n",
    "# Evaluation dataset should always stay the same\n",
    "INPUT_EVALUATION_FILE = \"../data/preprocessed_data/evaluation_dataset.csv\"\n",
    "\n",
    "# Output parameters\n",
    "METHOD_NAME = \"tf_idf_svm\"\n",
    "PREPROCESSOR_NAME = \"baseline_and_bayess_specific\"\n",
    "OUTPUT_MODEL = f\"../data/models/{METHOD_NAME}_model.pkl\"\n",
    "OUTPUT_RESULTS = f\"../data/results/{METHOD_NAME}_model.txt\"\n",
    "\n",
    "# Hyper parameter alternatives\n",
    "HYPER_PARAMETER_MIN_DF = list(range(0, 100, 10))\n",
    "HYPER_PARAMETER_MAX_DF = list(numpy.arange(0.01, 0.10, 0.02))\n",
    "HYPER_PARAMETER_C = list(numpy.arange(0.1, 1, 0.1))\n",
    "\n",
    "# Hyper parameter optimization parameters\n",
    "HYPER_PARAMETER_OPTIMIZATION_SCORING = \"accuracy\"\n",
    "HYPER_PARAMETER_OPTIMIZATION_CV = 5\n",
    "\n",
    "# Other constants\n",
    "LABELS = [\"negative\", \"positive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries for your machine learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets\n",
    "Note that the preprocessed data should contain at least the following fields:\n",
    "[prep_text],[sentiment]\n",
    "\n",
    "Loading training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_csv(INPUT_TRAINING_FILE, engine=\"python\", delimiter=\",\")\n",
    "training_tweets = training_dataset[\"prep_text\"].apply(\n",
    "    lambda tweet: str(tweet))\n",
    "training_sentiment_targets = training_dataset[\"sentiment\"].apply(\n",
    "    lambda sentiment: int(sentiment))\n",
    "\n",
    "evaluation_dataset = pd.read_csv(INPUT_EVALUATION_FILE, engine=\"python\", delimiter=\",\")\n",
    "evaluation_tweets = evaluation_dataset[\"prep_text\"].apply(\n",
    "    lambda tweet: str(tweet))\n",
    "evaluation_sentiment_targets = evaluation_dataset[\"sentiment\"].apply(\n",
    "    lambda sentiment: int(sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the preprocessor and do some preprocessing for the training dataset\n",
    "Preprocessing part should only include conversion techniques that are required by the algorithm. General preprocessing should be done in the separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove very short tweets from the training dataset\n",
    "mask = training_tweets.str.len() > 30\n",
    "training_tweets = training_tweets[mask]\n",
    "training_sentiment_targets = training_sentiment_targets[mask]\n",
    "\n",
    "# Define the count vectorizer with certain sanity limits\n",
    "preprocessor = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define rest of the pipeline\n",
    "Definition should include splitting of the data using cross validator and hyper parameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create internal pipeline\n",
    "classifier = LinearSVC()\n",
    "pipeline = Pipeline(steps=[(\"preprocessing\", preprocessor), (\"classification\", classifier)])\n",
    "\n",
    "# Specify the tunable hyper parameters\n",
    "parameters = {\n",
    "    \"preprocessing__min_df\": HYPER_PARAMETER_MIN_DF,\n",
    "    \"preprocessing__max_df\": HYPER_PARAMETER_MAX_DF,\n",
    "    \"classification__C\": HYPER_PARAMETER_C\n",
    "}\n",
    "\n",
    "# Define KFold parameters\n",
    "cv = StratifiedKFold(n_splits=HYPER_PARAMETER_OPTIMIZATION_CV, shuffle=True, random_state=42)\n",
    "\n",
    "estimator = GridSearchCV(pipeline, parameters,\n",
    "    scoring=HYPER_PARAMETER_OPTIMIZATION_SCORING, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(training_tweets, training_sentiment_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all training data to calculate confusion matrix for training data\n",
    "training_estimates = estimator.predict(training_tweets)\n",
    "training_accuracy = accuracy_score(training_sentiment_targets, training_estimates)\n",
    "training_confusion_matrix = confusion_matrix(training_sentiment_targets, training_estimates)\n",
    "training_classification_report = classification_report(training_sentiment_targets, training_estimates, output_dict=True, target_names=LABELS)\n",
    "\n",
    "# Use model to estimate manually labeled evaluation Tweets\n",
    "evaluation_estimates = estimator.predict(evaluation_tweets)\n",
    "evaluation_accuracy = accuracy_score(evaluation_sentiment_targets, evaluation_estimates)\n",
    "evaluation_confusion_matrix = confusion_matrix(evaluation_sentiment_targets, evaluation_estimates)\n",
    "evaluation_classification_report = classification_report(evaluation_sentiment_targets, evaluation_estimates, output_dict=True, target_names=LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained model for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_MODEL, \"wb\") as handle:\n",
    "    pickle.dump(estimator, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result statistics\n",
    "These should be always saved in the same fashion, so the results can be compared between different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_parameters:\n",
      "  classification__C: 0.5\n",
      "metadata:\n",
      "  estimator_name: LinearSVC()\n",
      "  method_name: tf_idf_svm\n",
      "  parameter_optimization:\n",
      "    hyper_parameter_optimization:\n",
      "      cv: 10\n",
      "      scoring: accuracy\n",
      "  preprocessor_name: baseline_and_bayess_specific\n",
      "scores:\n",
      "  evaluation_scores:\n",
      "    accuracy: 0.8542713567839196\n",
      "    classification_report:\n",
      "      accuracy: 0.8542713567839196\n",
      "      macro avg:\n",
      "        f1-score: 0.8539024328498013\n",
      "        precision: 0.8572884811416921\n",
      "        recall: 0.854040404040404\n",
      "        support: 199\n",
      "      negative:\n",
      "        f1-score: 0.8465608465608466\n",
      "        precision: 0.8888888888888888\n",
      "        recall: 0.8080808080808081\n",
      "        support: 99\n",
      "      positive:\n",
      "        f1-score: 0.861244019138756\n",
      "        precision: 0.8256880733944955\n",
      "        recall: 0.9\n",
      "        support: 100\n",
      "      weighted avg:\n",
      "        f1-score: 0.853939325243213\n",
      "        precision: 0.8571296851228621\n",
      "        recall: 0.8542713567839196\n",
      "        support: 199\n",
      "    confusion_matrix:\n",
      "      false_negative: 19\n",
      "      false_positive: 10\n",
      "      true_negative: 80\n",
      "      true_positive: 90\n",
      "  training_scores:\n",
      "    accuracy: 0.7106788831003602\n",
      "    classification_report:\n",
      "      accuracy: 0.7106788831003602\n",
      "      macro avg:\n",
      "        f1-score: 0.7106718844206135\n",
      "        precision: 0.7108058350990194\n",
      "        recall: 0.7107546302461833\n",
      "        support: 1575146\n",
      "      negative:\n",
      "        f1-score: 0.712094879439103\n",
      "        precision: 0.7034524507907186\n",
      "        recall: 0.7209523066354706\n",
      "        support: 781723\n",
      "      positive:\n",
      "        f1-score: 0.709248889402124\n",
      "        precision: 0.7181592194073201\n",
      "        recall: 0.700556953856896\n",
      "        support: 793423\n",
      "      weighted avg:\n",
      "        f1-score: 0.7106613145802836\n",
      "        precision: 0.7108604551764008\n",
      "        recall: 0.7106788831003602\n",
      "        support: 1575146\n",
      "    confusion_matrix:\n",
      "      false_negative: 218138\n",
      "      false_positive: 237585\n",
      "      true_negative: 563585\n",
      "      true_positive: 555838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary object, where results will be accumulated\n",
    "result_dict= {}\n",
    "\n",
    "# Metadata section\n",
    "metadata_dict = {}\n",
    "metadata_dict[\"preprocessor_name\"] = PREPROCESSOR_NAME\n",
    "metadata_dict[\"method_name\"] = METHOD_NAME\n",
    "metadata_dict[\"estimator_name\"] = str(classifier)\n",
    "result_dict[\"metadata\"] = metadata_dict\n",
    "\n",
    "# Hyper parameter optimization values\n",
    "hyper_parameter_optimization_dict = {}\n",
    "hyper_parameter_optimization_dict[\"scoring\"] = HYPER_PARAMETER_OPTIMIZATION_SCORING\n",
    "hyper_parameter_optimization_dict[\"cv\"] = HYPER_PARAMETER_OPTIMIZATION_CV\n",
    "\n",
    "parameter_optimization_dict = {}\n",
    "parameter_optimization_dict[\"hyper_parameter_optimization\"] = hyper_parameter_optimization_dict\n",
    "metadata_dict[\"parameter_optimization\"] = parameter_optimization_dict\n",
    "\n",
    "# Save best parameters\n",
    "result_dict[\"best_parameters\"] = estimator.best_params_\n",
    "\n",
    "# Different kind of scores\n",
    "scores_dict = {}\n",
    "\n",
    "training_scores_dict = {}\n",
    "training_scores_dict[\"accuracy\"] = float(training_accuracy)\n",
    "training_confusion_matrix_dict = {}\n",
    "training_confusion_matrix_dict[\"true_negative\"] = int(training_confusion_matrix[0][0])\n",
    "training_confusion_matrix_dict[\"true_positive\"] = int(training_confusion_matrix[1][1])\n",
    "training_confusion_matrix_dict[\"false_negative\"] = int(training_confusion_matrix[0][1])\n",
    "training_confusion_matrix_dict[\"false_positive\"] = int(training_confusion_matrix[1][0])\n",
    "training_scores_dict[\"confusion_matrix\"] = training_confusion_matrix_dict\n",
    "training_scores_dict[\"classification_report\"] = training_classification_report\n",
    "scores_dict[\"training_scores\"] = training_scores_dict\n",
    "\n",
    "evaluation_scores_dict = {}\n",
    "evaluation_scores_dict[\"accuracy\"] = float(evaluation_accuracy)\n",
    "evaluation_confusion_matrix_dict = {}\n",
    "evaluation_confusion_matrix_dict[\"true_negative\"] = int(evaluation_confusion_matrix[0][0])\n",
    "evaluation_confusion_matrix_dict[\"true_positive\"] = int(evaluation_confusion_matrix[1][1])\n",
    "evaluation_confusion_matrix_dict[\"false_negative\"] = int(evaluation_confusion_matrix[0][1])\n",
    "evaluation_confusion_matrix_dict[\"false_positive\"] = int(evaluation_confusion_matrix[1][0])\n",
    "evaluation_scores_dict[\"confusion_matrix\"] = evaluation_confusion_matrix_dict\n",
    "evaluation_scores_dict[\"classification_report\"] = evaluation_classification_report\n",
    "scores_dict[\"evaluation_scores\"] = evaluation_scores_dict\n",
    "\n",
    "result_dict[\"scores\"] = scores_dict\n",
    "\n",
    "# Convert statistics to pretty YAML\n",
    "results = yaml.dump(result_dict)\n",
    "\n",
    "# Print results\n",
    "print(results)\n",
    "\n",
    "# Save results to the file\n",
    "with open(OUTPUT_RESULTS, \"w\") as file:\n",
    "    file.write(results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df7a9a5c37ddd52c7652a98db41541d6c37368917739ba33de99dadb21ef70fe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
