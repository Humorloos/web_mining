{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define local constants\n",
    "Change these constants based on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Input data path\n",
    "INPUT_TRAINING_FILE = \"../data/preprocessed_data/training_dataset.csv\"\n",
    "# Evaluation dataset should always stay the same\n",
    "INPUT_EVALUATION_FILE = \"../data/preprocessed_data/evaluation_dataset.csv\"\n",
    "\n",
    "# Output parameters\n",
    "METHOD_NAME = \"tf_idf_knn\"\n",
    "PREPROCESSOR_NAME = \"baseline_and_bayess_specific\"\n",
    "OUTPUT_MODEL = f\"../data/models/{METHOD_NAME}_model.pkl\"\n",
    "OUTPUT_RESULTS = f\"../data/results/{METHOD_NAME}_model.txt\"\n",
    "\n",
    "# Hyper parameter alternatives\n",
    "HYPER_PARAMETER_MIN_DF = list(range(0, 100, 10))\n",
    "HYPER_PARAMETER_MAX_DF = list(numpy.arange(0.01, 0.10, 0.01))\n",
    "HYPER_PARAMETER_MAX_FEATURES = list(range(5000, 100000, 5000))\n",
    "HYPER_PARAMETER_USE_IDF = [False, True]\n",
    "\n",
    "# Hyper parameter optimization parameters\n",
    "HYPER_PARAMETER_OPTIMIZATION_SCORING = \"accuracy\"\n",
    "HYPER_PARAMETER_OPTIMIZATION_CV = 2\n",
    "\n",
    "# Other constants\n",
    "LABELS = [\"negative\", \"positive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries for your machine learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets\n",
    "Note that the preprocessed data should contain at least the following fields:\n",
    "[prep_text],[sentiment]\n",
    "\n",
    "Loading training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_csv(INPUT_TRAINING_FILE, engine=\"python\", delimiter=\",\")\n",
    "training_tweets = training_dataset[\"prep_text\"].apply(\n",
    "    lambda tweet: str(tweet))\n",
    "training_sentiment_targets = training_dataset[\"sentiment\"].apply(\n",
    "    lambda sentiment: int(sentiment))\n",
    "\n",
    "evaluation_dataset = pd.read_csv(INPUT_EVALUATION_FILE, engine=\"python\", delimiter=\",\")\n",
    "evaluation_tweets = evaluation_dataset[\"prep_text\"].apply(\n",
    "    lambda tweet: str(tweet))\n",
    "evaluation_sentiment_targets = evaluation_dataset[\"sentiment\"].apply(\n",
    "    lambda sentiment: int(sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the preprocessor and do some preprocessing for the training dataset\n",
    "Preprocessing part should only include conversion techniques that are required by the algorithm. General preprocessing should be done in the separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove very short tweets from the training dataset\n",
    "mask = training_tweets.str.len() > 30\n",
    "training_tweets = training_tweets[mask]\n",
    "training_sentiment_targets = training_sentiment_targets[mask]\n",
    "\n",
    "# Define the count vectorizer with certain sanity limits\n",
    "preprocessor = TfidfVectorizer(min_df=20, max_df=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define rest of the pipeline\n",
    "Definition should include splitting of the data using cross validator and hyper parameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create internal pipeline\n",
    "classifier = KNeighborsClassifier(n_neighbors=2)\n",
    "pipeline = Pipeline(steps=[(\"preprocessing\", preprocessor), (\"classification\", classifier)])\n",
    "\n",
    "# Specify the tunable hyper parameters\n",
    "parameters = {\n",
    "    #\"preprocessing__min_df\": HYPER_PARAMETER_MIN_DF,\n",
    "    #\"preprocessing__max_df\": HYPER_PARAMETER_MAX_DF,\n",
    "    #\"preprocessing__max_features\": HYPER_PARAMETER_MAX_FEATURES,\n",
    "    #\"preprocessing__use_idf\": HYPER_PARAMETER_USE_IDF\n",
    "}\n",
    "\n",
    "# Define KFold parameters\n",
    "cv = StratifiedKFold(n_splits=HYPER_PARAMETER_OPTIMIZATION_CV, shuffle=True, random_state=42)\n",
    "\n",
    "estimator = GridSearchCV(pipeline, parameters,\n",
    "    scoring=HYPER_PARAMETER_OPTIMIZATION_SCORING, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = estimator.fit(training_tweets, training_sentiment_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51767/2432109160.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use all training data to calculate confusion matrix for training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_estimates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtraining_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sentiment_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_estimates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtraining_confusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sentiment_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_estimates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtraining_classification_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sentiment_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_estimates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mavailable_if\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_final_estimator_has\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit_predict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mdata\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \"\"\"\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m             chunked_results = list(\n\u001b[0m\u001b[1;32m    753\u001b[0m                 pairwise_distances_chunked(\n\u001b[1;32m    754\u001b[0m                     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1724\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduce_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m             \u001b[0mchunk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1726\u001b[0;31m             \u001b[0mD_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1727\u001b[0m             \u001b[0m_check_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m_kneighbors_reduce_func\u001b[0;34m(self, dist, start, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \"\"\"\n\u001b[1;32m    633\u001b[0m         \u001b[0msample_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m         \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneigh_ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;31m# argpartition doesn't guarantee sorted order, so we sort again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margpartition\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margpartition\u001b[0;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \"\"\"\n\u001b[0;32m--> 837\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argpartition'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use all training data to calculate confusion matrix for training data\n",
    "training_estimates = estimator.predict(training_tweets)\n",
    "training_accuracy = accuracy_score(training_sentiment_targets, training_estimates)\n",
    "training_confusion_matrix = confusion_matrix(training_sentiment_targets, training_estimates)\n",
    "training_classification_report = classification_report(training_sentiment_targets, training_estimates, output_dict=True, target_names=LABELS)\n",
    "\n",
    "# Use model to estimate manually labeled evaluation Tweets\n",
    "evaluation_estimates = estimator.predict(evaluation_tweets)\n",
    "evaluation_accuracy = accuracy_score(evaluation_sentiment_targets, evaluation_estimates)\n",
    "evaluation_confusion_matrix = confusion_matrix(evaluation_sentiment_targets, evaluation_estimates)\n",
    "evaluation_classification_report = classification_report(evaluation_sentiment_targets, evaluation_estimates, output_dict=True, target_names=LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained model for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_MODEL, \"wb\") as handle:\n",
    "    pickle.dump(estimator, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result statistics\n",
    "These should be always saved in the same fashion, so the results can be compared between different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_parameters:\n",
      "  classification__C: 0.5\n",
      "metadata:\n",
      "  estimator_name: LinearSVC()\n",
      "  method_name: tf_idf_svm\n",
      "  parameter_optimization:\n",
      "    hyper_parameter_optimization:\n",
      "      cv: 10\n",
      "      scoring: accuracy\n",
      "  preprocessor_name: baseline_and_bayess_specific\n",
      "scores:\n",
      "  evaluation_scores:\n",
      "    accuracy: 0.8542713567839196\n",
      "    classification_report:\n",
      "      accuracy: 0.8542713567839196\n",
      "      macro avg:\n",
      "        f1-score: 0.8539024328498013\n",
      "        precision: 0.8572884811416921\n",
      "        recall: 0.854040404040404\n",
      "        support: 199\n",
      "      negative:\n",
      "        f1-score: 0.8465608465608466\n",
      "        precision: 0.8888888888888888\n",
      "        recall: 0.8080808080808081\n",
      "        support: 99\n",
      "      positive:\n",
      "        f1-score: 0.861244019138756\n",
      "        precision: 0.8256880733944955\n",
      "        recall: 0.9\n",
      "        support: 100\n",
      "      weighted avg:\n",
      "        f1-score: 0.853939325243213\n",
      "        precision: 0.8571296851228621\n",
      "        recall: 0.8542713567839196\n",
      "        support: 199\n",
      "    confusion_matrix:\n",
      "      false_negative: 19\n",
      "      false_positive: 10\n",
      "      true_negative: 80\n",
      "      true_positive: 90\n",
      "  training_scores:\n",
      "    accuracy: 0.7106788831003602\n",
      "    classification_report:\n",
      "      accuracy: 0.7106788831003602\n",
      "      macro avg:\n",
      "        f1-score: 0.7106718844206135\n",
      "        precision: 0.7108058350990194\n",
      "        recall: 0.7107546302461833\n",
      "        support: 1575146\n",
      "      negative:\n",
      "        f1-score: 0.712094879439103\n",
      "        precision: 0.7034524507907186\n",
      "        recall: 0.7209523066354706\n",
      "        support: 781723\n",
      "      positive:\n",
      "        f1-score: 0.709248889402124\n",
      "        precision: 0.7181592194073201\n",
      "        recall: 0.700556953856896\n",
      "        support: 793423\n",
      "      weighted avg:\n",
      "        f1-score: 0.7106613145802836\n",
      "        precision: 0.7108604551764008\n",
      "        recall: 0.7106788831003602\n",
      "        support: 1575146\n",
      "    confusion_matrix:\n",
      "      false_negative: 218138\n",
      "      false_positive: 237585\n",
      "      true_negative: 563585\n",
      "      true_positive: 555838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary object, where results will be accumulated\n",
    "result_dict= {}\n",
    "\n",
    "# Metadata section\n",
    "metadata_dict = {}\n",
    "metadata_dict[\"preprocessor_name\"] = PREPROCESSOR_NAME\n",
    "metadata_dict[\"method_name\"] = METHOD_NAME\n",
    "metadata_dict[\"estimator_name\"] = str(classifier)\n",
    "result_dict[\"metadata\"] = metadata_dict\n",
    "\n",
    "# Hyper parameter optimization values\n",
    "hyper_parameter_optimization_dict = {}\n",
    "hyper_parameter_optimization_dict[\"scoring\"] = HYPER_PARAMETER_OPTIMIZATION_SCORING\n",
    "hyper_parameter_optimization_dict[\"cv\"] = HYPER_PARAMETER_OPTIMIZATION_CV\n",
    "\n",
    "parameter_optimization_dict = {}\n",
    "parameter_optimization_dict[\"hyper_parameter_optimization\"] = hyper_parameter_optimization_dict\n",
    "metadata_dict[\"parameter_optimization\"] = parameter_optimization_dict\n",
    "\n",
    "# Save best parameters\n",
    "result_dict[\"best_parameters\"] = estimator.best_params_\n",
    "\n",
    "# Different kind of scores\n",
    "scores_dict = {}\n",
    "\n",
    "training_scores_dict = {}\n",
    "training_scores_dict[\"accuracy\"] = float(training_accuracy)\n",
    "training_confusion_matrix_dict = {}\n",
    "training_confusion_matrix_dict[\"true_negative\"] = int(training_confusion_matrix[0][0])\n",
    "training_confusion_matrix_dict[\"true_positive\"] = int(training_confusion_matrix[1][1])\n",
    "training_confusion_matrix_dict[\"false_negative\"] = int(training_confusion_matrix[0][1])\n",
    "training_confusion_matrix_dict[\"false_positive\"] = int(training_confusion_matrix[1][0])\n",
    "training_scores_dict[\"confusion_matrix\"] = training_confusion_matrix_dict\n",
    "training_scores_dict[\"classification_report\"] = training_classification_report\n",
    "scores_dict[\"training_scores\"] = training_scores_dict\n",
    "\n",
    "evaluation_scores_dict = {}\n",
    "evaluation_scores_dict[\"accuracy\"] = float(evaluation_accuracy)\n",
    "evaluation_confusion_matrix_dict = {}\n",
    "evaluation_confusion_matrix_dict[\"true_negative\"] = int(evaluation_confusion_matrix[0][0])\n",
    "evaluation_confusion_matrix_dict[\"true_positive\"] = int(evaluation_confusion_matrix[1][1])\n",
    "evaluation_confusion_matrix_dict[\"false_negative\"] = int(evaluation_confusion_matrix[0][1])\n",
    "evaluation_confusion_matrix_dict[\"false_positive\"] = int(evaluation_confusion_matrix[1][0])\n",
    "evaluation_scores_dict[\"confusion_matrix\"] = evaluation_confusion_matrix_dict\n",
    "evaluation_scores_dict[\"classification_report\"] = evaluation_classification_report\n",
    "scores_dict[\"evaluation_scores\"] = evaluation_scores_dict\n",
    "\n",
    "result_dict[\"scores\"] = scores_dict\n",
    "\n",
    "# Convert statistics to pretty YAML\n",
    "results = yaml.dump(result_dict)\n",
    "\n",
    "# Print results\n",
    "print(results)\n",
    "\n",
    "# Save results to the file\n",
    "with open(OUTPUT_RESULTS, \"w\") as file:\n",
    "    file.write(results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df7a9a5c37ddd52c7652a98db41541d6c37368917739ba33de99dadb21ef70fe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
