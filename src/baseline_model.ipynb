{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define local constants\n",
    "Change these constants based on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data path\n",
    "INPUT_TRAINING_FILE = \"../data/preprocessed_data/tweets_combined_preprocessed_1190k.csv\"\n",
    "# Evaluation dataset should always stay the same\n",
    "INPUT_EVALUATION_FILE = \"../data/preprocessed_data/verification_dataset.csv\"\n",
    "\n",
    "# Output parameters\n",
    "METHOD_NAME = \"baseline\"\n",
    "PREPROCESSOR_NAME = \"baseline\"\n",
    "OUTPUT_MODEL = f\"../data/models/{METHOD_NAME}_model.pkl\"\n",
    "OUTPUT_RESULTS = f\"../data/results/{METHOD_NAME}_model.txt\"\n",
    "\n",
    "# Hyper parameter alternatives\n",
    "HYPER_PARAMETER_KERNEL = [\"rfb\"]\n",
    "HYPER_PARAMETER_GAMMA = [\"scale\"]\n",
    "\n",
    "# Hyper parameter optimization parameters\n",
    "HYPER_PARAMETER_OPTIMIZATION_SCORING = \"accuracy\"\n",
    "HYPER_PARAMETER_OPTIMIZATION_CV = 2\n",
    "\n",
    "# Cross validation parameters\n",
    "CROSS_VALIDATION_SCORING = \"accuracy\"\n",
    "CROSS_VALIDATION_CV = 5\n",
    "\n",
    "# Other constants\n",
    "LABELS = [\"negative\", \"positive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries for your machine learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets\n",
    "Note that the preprocessed data should contain at least the following fields:\n",
    "[prep_text],[sentiment]\n",
    "\n",
    "Loading training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_csv(INPUT_TRAINING_FILE, engine=\"python\", delimiter=\",\")\n",
    "training_tweets = training_dataset[\"prep_text\"].apply(\n",
    "    lambda tweet: str(tweet))\n",
    "training_sentiment_targets = training_dataset[\"sentiment\"].apply(\n",
    "    lambda sentiment: int(sentiment))\n",
    "\n",
    "evaluation_dataset = pd.read_csv(INPUT_EVALUATION_FILE, engine=\"python\", delimiter=\",\")\n",
    "evaluation_tweets = evaluation_dataset[\"prep_text\"].apply(\n",
    "    lambda tweet: str(tweet))\n",
    "evaluation_sentiment_targets = evaluation_dataset[\"sentiment\"].apply(\n",
    "    lambda sentiment: int(sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the preprocessor\n",
    "Preprocessing part should only include conversion techniques that are required by the algorithm. General preprocessing should be done in the separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define rest of the pipeline\n",
    "Definition should include splitting of the data using cross validator and hyper parameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create internal pipeline\n",
    "classifier = svm.SVC()\n",
    "pipeline = Pipeline(steps=[(\"preprocessing\", preprocessor), (\"classification\", classifier)])\n",
    "\n",
    "# Specify the tunable hyper parameters\n",
    "parameters = {\n",
    "    \"classification__kernel\": HYPER_PARAMETER_KERNEL,\n",
    "    \"classification__gamma\": HYPER_PARAMETER_GAMMA\n",
    "}\n",
    "\n",
    "# Define KFold parameters\n",
    "cv = StratifiedKFold(n_splits=HYPER_PARAMETER_OPTIMIZATION_CV, shuffle=True, random_state=42)\n",
    "\n",
    "estimator = GridSearchCV(pipeline, parameters,\n",
    "    scoring=HYPER_PARAMETER_OPTIMIZATION_SCORING, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roni/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/roni/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/roni/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/roni/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/roni/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/roni/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/roni/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/roni/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/roni/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/roni/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/roni/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/roni/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/roni/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/roni/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/roni/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = cross_val_score(estimator, training_tweets, training_sentiment_targets,\n",
    "    scoring=CROSS_VALIDATION_SCORING, cv=CROSS_VALIDATION_CV, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23801/159138393.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use all training data to calculate confusion matrix for training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_estimates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtraining_confusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sentiment_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_estimates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtraining_classification_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sentiment_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_estimates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mbest\u001b[0m \u001b[0mfound\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \"\"\"\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Use all training data to calculate confusion matrix for training data\n",
    "training_estimates = estimator.predict(training_tweets)\n",
    "training_confusion_matrix = confusion_matrix(training_sentiment_targets, training_estimates)\n",
    "training_classification_report = classification_report(training_sentiment_targets, training_estimates, target_names=LABELS)\n",
    "\n",
    "# Use model to estimate manually labeled evaluation Tweets\n",
    "evaluation_estimates = estimator.predict(evaluation_tweets)\n",
    "evaluation_accuracy = accuracy_score(evaluation_sentiment_targets, evaluation_estimates)\n",
    "evaluation_confusion_matrix = confusion_matrix(evaluation_sentiment_targets, evaluation_estimates)\n",
    "evaluation_classification_report = classification_report(evaluation_sentiment_targets, evaluation_estimates, target_names=LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained model for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_MODEL, \"wb\") as handle:\n",
    "    pickle.dump(estimator, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result statistics\n",
    "These should be always saved in the same fashion, so the results can be compared between different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary object, where results will be accumulated\n",
    "result_dict= {}\n",
    "\n",
    "# Metadata section\n",
    "metadata_dict = {}\n",
    "metadata_dict[\"preprocessor_name\"] = PREPROCESSOR_NAME\n",
    "metadata_dict[\"method_name\"] = METHOD_NAME\n",
    "metadata_dict[\"estimator_name\"] = classifier.__name__\n",
    "result_dict[\"metadata\"] = metadata_dict\n",
    "\n",
    "# Hyper parameter optimization values\n",
    "hyper_parameter_optimization_dict = {}\n",
    "hyper_parameter_optimization_dict[\"scoring\"] = HYPER_PARAMETER_OPTIMIZATION_SCORING\n",
    "hyper_parameter_optimization_dict[\"cv\"] = HYPER_PARAMETER_OPTIMIZATION_CV\n",
    "\n",
    "# Cross validation parameters\n",
    "cross_validation_dict = {}\n",
    "cross_validation_dict[\"scoring\"] = CROSS_VALIDATION_SCORING\n",
    "cross_validation_dict[\"cv\"] = CROSS_VALIDATION_CV\n",
    "\n",
    "parameter_optimization_dict = {}\n",
    "parameter_optimization_dict[\"hyper_parameter_optimization\"] = hyper_parameter_optimization_dict\n",
    "parameter_optimization_dict[\"cross_validation_parameters\"] = cross_validation_dict\n",
    "metadata_dict[\"parameter_optimization\"] = parameter_optimization_dict\n",
    "\n",
    "# Combine parameters under parameters dict\n",
    "parameters_dict = {}\n",
    "parameters_dict[\"hyper_parameter_optimization_values\"] = hyper_parameter_optimization_dict\n",
    "parameters_dict[\"cross_validation_parameters\"] =  cross_validation_dict\n",
    "parameters_dict[\"selected_hyper_parameters\"] = estimator.best_params_\n",
    "result_dict[\"parameters\"] = parameters_dict\n",
    "\n",
    "# Different kind of scores\n",
    "scores_dict = {}\n",
    "\n",
    "training_scores_dict = {}\n",
    "training_scores_dict[\"accuracy\"] = training_accuracy\n",
    "training_confusion_matrix_dict = {}\n",
    "training_confusion_matrix_dict[\"true_negative\"] = training_confusion_matrix[1][0]\n",
    "training_confusion_matrix_dict[\"true_positive\"] = training_confusion_matrix[1][1]\n",
    "training_confusion_matrix_dict[\"false_negative\"] = training_confusion_matrix[0][0]\n",
    "training_confusion_matrix_dict[\"false_positive\"] = training_confusion_matrix[0][1]\n",
    "training_scores_dict[\"confusion_matrix\"] = training_confusion_matrix_dict\n",
    "training_scores_dict[\"classification_report\"] = str(training_classification_report)\n",
    "scores_dict[\"training_scores\"] = training_scores_dict\n",
    "\n",
    "evaluation_scores_dict = {}\n",
    "evaluation_scores_dict[\"accuracy\"] = training_accuracy\n",
    "evaluation_confusion_matrix_dict = {}\n",
    "evaluation_confusion_matrix_dict[\"true_negative\"] = evaluation_confusion_matrix[1][0]\n",
    "evaluation_confusion_matrix_dict[\"true_positive\"] = evaluation_confusion_matrix[1][1]\n",
    "evaluation_confusion_matrix_dict[\"false_negative\"] = evaluation_confusion_matrix[0][0]\n",
    "evaluation_confusion_matrix_dict[\"false_positive\"] = evaluation_confusion_matrix[0][1]\n",
    "evaluation_scores_dict[\"confusion_matrix\"] = evaluation_confusion_matrix_dict\n",
    "training_scores_dict[\"classification_report\"] = str(evaluation_classification_report)\n",
    "scores_dict[\"evaluation_scores\"] = evaluation_scores_dict\n",
    "\n",
    "result_dict[\"scores\"] = scores_dict\n",
    "\n",
    "# Convert statistics to pretty YAML\n",
    "results = yaml.dump(result_dict)\n",
    "\n",
    "# Print results\n",
    "print(results)\n",
    "\n",
    "# Save results to the file\n",
    "with open(OUTPUT_RESULTS, \"r\") as file:\n",
    "    file.write(results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df7a9a5c37ddd52c7652a98db41541d6c37368917739ba33de99dadb21ef70fe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
